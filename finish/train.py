import torch
import numpy as np
from diffusers import StableDiffusionPipeline
from diffusers.optimization import get_cosine_schedule_with_warmup
from torch.utils.data import Dataset
import os
from PIL import Image
from tqdm import tqdm
import json
from peft import LoraConfig, get_peft_model
from torch.utils.data import DataLoader
import traceback

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
class Config:
    pretrained_model = "runwayml/stable-diffusion-v1-5"
    image_dir = "./Attempt 4/data/images"
    prompt_json = "./Attempt 4/data/captions.json"  # –§–∞–π–ª —Å –ø—Ä–æ–º—Ç–∞–º–∏
    output_dir = "./lora_output_7"
    resolution = 512
    batch_size = 3
    gradient_accumulation_steps = 10
    learning_rate = 1e-3
    lr_warmup_steps = 100
    num_epochs = 400
    lora_rank = 5
    save_steps = 50

class TextImageDataset(Dataset):
    def __init__(self, image_dir, prompt_json, resolution=512):
        try:
            self.image_paths = []
            self.prompts = {}
            self.resolution = resolution
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if not os.path.exists(image_dir):
                raise FileNotFoundError(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {image_dir}")
            
            valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp')
            self.image_paths = [
                os.path.join(image_dir, f) 
                for f in os.listdir(image_dir) 
                if f.lower().endswith(valid_extensions)
            ]
            
            if not self.image_paths:
                raise ValueError(f"–í –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ {image_dir} –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏ {valid_extensions}")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ JSON —Å –ø—Ä–æ–º—Ç–∞–º–∏
            if not os.path.exists(prompt_json):
                raise FileNotFoundError(f"–§–∞–π–ª —Å –ø—Ä–æ–º—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {prompt_json}")
            
            with open(prompt_json, 'r', encoding='utf-8') as f:
                self.prompts = json.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –ø—Ä–æ–º—Ç–æ–≤
            missing_prompts = [
                os.path.basename(img_path) 
                for img_path in self.image_paths 
                if os.path.basename(img_path) not in self.prompts
            ]
            
            if missing_prompts:
                print(f"\n‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –¥–ª—è {len(missing_prompts)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ—Ç –ø—Ä–æ–º—Ç–æ–≤!")
                print("–ü—Ä–∏–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –ø—Ä–æ–º—Ç–æ–≤:", missing_prompts[:5])
                
        except Exception as e:
            print("\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
            print(f"–¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
            print(f"–°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)}")
            print("\n–°—Ç–µ–∫ –≤—ã–∑–æ–≤–æ–≤:")
            traceback.print_exc()
            raise

    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        try:
            img_path = self.image_paths[idx]
            filename = os.path.basename(img_path)
            prompt = self.prompts.get(filename, "")
            
            if not prompt:
                print(f"‚ö†Ô∏è –î–ª—è —Ñ–∞–π–ª–∞ {filename} –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—É—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç")
            
            img = Image.open(img_path).convert("RGB")
            img = img.resize((self.resolution, self.resolution))
            img = np.array(img).astype(np.float32) / 127.5 - 1.0
            img = torch.from_numpy(img).permute(2, 0, 1)
            
            return {"pixel_values": img, "prompt": prompt}
            
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {img_path}:")
            print(f"–¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
            print(f"–°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)}")
            raise

def setup_model(config):
    try:
        print("\nüîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...")
        pipe = StableDiffusionPipeline.from_pretrained(
            config.pretrained_model, 
            torch_dtype=torch.float16,
            safety_checker=None
        ).to("cuda")
        
        print("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA...")
        lora_config = LoraConfig(
            r=config.lora_rank,
            lora_alpha=16,
            target_modules=["to_k", "to_q", "to_v", "proj_out"],
            init_lora_weights="gaussian",
        )
        
        unet = pipe.unet
        unet = get_peft_model(unet, lora_config)
        unet.print_trainable_parameters()
        
        pipe.vae = pipe.vae.half()
        
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        return pipe, unet
        
    except Exception as e:
        print("\n‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏:")
        print(f"–¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
        print(f"–°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)}")
        print("\n–°—Ç–µ–∫ –≤—ã–∑–æ–≤–æ–≤:")
        traceback.print_exc()
        raise

def train_lora():
    try:
        config = Config()
        os.makedirs(config.output_dir, exist_ok=True)
        
        print("\nüìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:")
        print(f"–ú–æ–¥–µ–ª—å: {config.pretrained_model}")
        print(f"–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {config.batch_size}")
        print(f"–®–∞–≥–æ–≤ –∞–∫–∫—É–º—É–ª—è—Ü–∏–∏: {config.gradient_accumulation_steps}")
        print(f"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {config.batch_size * config.gradient_accumulation_steps}")
        print(f"–ö–∞—Ç–∞–ª–æ–≥ –≤—ã–≤–æ–¥–∞: {os.path.abspath(config.output_dir)}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        dataset = TextImageDataset(config.image_dir, config.prompt_json, config.resolution)
        dataloader = DataLoader(
            dataset,
            batch_size=config.batch_size,
            shuffle=True,
            collate_fn=lambda batch: {
                'pixel_values': torch.stack([item['pixel_values'] for item in batch]),
                'prompt': [item['prompt'] for item in batch]
            }
        )
        
        pipe, unet = setup_model(config)
        optimizer = torch.optim.AdamW(unet.parameters(), lr=config.learning_rate)
        
        total_steps = (len(dataloader) * config.num_epochs) // config.gradient_accumulation_steps
        lr_scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=config.lr_warmup_steps,
            num_training_steps=total_steps
        )
        
        # –û–±—É—á–µ–Ω–∏–µ
        global_step = 0
        actual_steps_since_save = 0
        
        print("\nüöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        for epoch in range(config.num_epochs):
            progress_bar = tqdm(dataloader, desc=f"–≠–ø–æ—Ö–∞ {epoch+1}/{config.num_epochs}")
            
            for batch in progress_bar:
                try:
                    # Forward pass
                    pixel_values = batch['pixel_values'].half().to("cuda")
                    
                    text_inputs = pipe.tokenizer(
                        batch['prompt'],
                        padding="max_length",
                        max_length=pipe.tokenizer.model_max_length,
                        return_tensors="pt",
                        truncation=True
                    ).to("cuda")
                    
                    with torch.no_grad():
                        text_embeddings = pipe.text_encoder(text_inputs.input_ids)[0]
                    
                    latents = pipe.vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * pipe.vae.config.scaling_factor
                    
                    noise = torch.randn_like(latents)
                    timesteps = torch.randint(
                        0, pipe.scheduler.config.num_train_timesteps, 
                        (latents.shape[0],), device="cuda"
                    )
                    noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)
                    
                    noise_pred = unet(noisy_latents, timesteps, text_embeddings).sample
                    loss = torch.nn.functional.mse_loss(noise_pred, noise)
                    loss = loss / config.gradient_accumulation_steps
                    loss.backward()
                    
                    actual_steps_since_save += 1
                    
                    # –®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                    if actual_steps_since_save % config.gradient_accumulation_steps == 0:
                        optimizer.step()
                        lr_scheduler.step()
                        optimizer.zero_grad()
                        global_step += 1
                        
                        progress_bar.set_postfix({
                            "loss": f"{loss.item() * config.gradient_accumulation_steps:.4f}",
                            "lr": f"{lr_scheduler.get_last_lr()[0]:.2e}",
                            "—à–∞–≥": global_step
                        })
                        
                        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                        if global_step % config.save_steps == 0:
                            save_path = os.path.join(config.output_dir, f"step_{global_step}")
                            unet.save_pretrained(save_path)
                            print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —à–∞–≥–µ {global_step} –≤ {save_path}")
                
                except Exception as e:
                    print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—à–∞–≥ {global_step}):")
                    print(f"–¢–∏–ø: {type(e).__name__}")
                    print(f"–°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)}")
                    print("\n–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –±–∞—Ç—á–∞...")
                    optimizer.zero_grad()  # –°–±—Ä–æ—Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
                    continue
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        final_path = os.path.join(config.output_dir, "final_lora")
        unet.save_pretrained(final_path)
        print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_path}")
        
    except Exception as e:
        print("\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ–±—É—á–µ–Ω–∏–∏:")
        print(f"–¢–∏–ø: {type(e).__name__}")
        print(f"–°–æ–æ–±—â–µ–Ω–∏–µ: {str(e)}")
        print("\n–°—Ç–µ–∫ –≤—ã–∑–æ–≤–æ–≤:")
        traceback.print_exc()
        raise

if __name__ == "__main__":
    train_lora()